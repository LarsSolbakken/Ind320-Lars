{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2719fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "!java -version\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Elhub-2021-Prod\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")   # match docker ps\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c03bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# Make a list of months in 2021\n",
    "months = pd.date_range(\"2021-01-01\", \"2021-12-31\", freq=\"MS\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for start in months:\n",
    "    end = start + pd.offsets.MonthEnd(1)   # <-- keep as datetime\n",
    "    \n",
    "    # Build URL params\n",
    "    params = {\n",
    "        \"dataset\": \"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startDate\": start.strftime(\"%Y-%m-%d\"),\n",
    "        \"endDate\": end.strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "\n",
    "    print(f\"Fetching {params['startDate']} to {params['endDate']}...\")\n",
    "\n",
    "    r = requests.get(base_url, params=params)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # import json\n",
    "    # print(json.dumps(data, indent=2)[:1000]) \n",
    "    # Flatten \"productionPerGroupMbaHour\"\n",
    "    records = []\n",
    "    for item in data[\"data\"]:\n",
    "        prod = item[\"attributes\"].get(\"productionPerGroupMbaHour\", [])\n",
    "        records.extend(prod)\n",
    "\n",
    "    all_data.extend(records)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18451f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Parse time safely (strings like \"2021-01-01T00:00:00+01:00\" -> UTC)\n",
    "for c in [\"startTime\"]:\n",
    "    df[c] = pd.to_datetime(df[c], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Cassandra's TIMESTAMP is UTC with no tz-info â†’ make tz-naive UTC\n",
    "df[\"startTime\"] = df[\"startTime\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "# Keep only the columns we want in Cassandra\n",
    "df4 = df[[\"priceArea\", \"productionGroup\", \"startTime\", \"quantityKwh\"]].copy()\n",
    "\n",
    "print(df4.head(), df4.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823732d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_spark = spark.createDataFrame(df4)\n",
    "\n",
    "# Optional: increase partitions a bit before write (tune if needed)\n",
    "df_spark = df_spark.repartition(8, \"priceArea\", \"productionGroup\")\n",
    "\n",
    "(df_spark.write\n",
    "   .format(\"org.apache.spark.sql.cassandra\")\n",
    "   .options(keyspace=\"elhub2021\", table=\"prod_by_group_hour\")\n",
    "   .mode(\"append\")                 # safe to re-run; same PK rows will overwrite\n",
    "   .save())\n",
    "\n",
    "print(\"Wrote rows to Cassandra:\", df_spark.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
